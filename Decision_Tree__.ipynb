{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree |"
      ],
      "metadata": {
        "id": "EWPpCkPn9sM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "  - A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. In the context of classification, it is a tree-like model used to predict the class or category of a target variable by learning simple decision rules inferred from data features.\n",
        "  - Think of it as a flowchart where the model asks a series of questions to norrow down the possibilities until it reaches a conclusion.\n",
        "  - How it works in Classification:\n",
        "    - The goal of a decision tree in classification is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
        "    1. Recursive Partitioning\n",
        "    - The process starts at the Root Node with the complete dataset. The algorithm looks for the \"best\" feature to split the data.\n",
        "      - It asks a question (e.g., \"Is the humidity > 70%\")\n",
        "      - Based on the answer, it splits the data into subsets.\n",
        "      - It then repeats this process for each subset (child node), choosing tghe best feature to split that specific subset.\n",
        "    2. Selecting the \"Best\" split\n",
        "    - How does the tree decides which feature to split on? It uses the mathematical metrics to measures the \"purity\" of the split. THe goal is to produce nodes that are as homogenous as possible(containing mostly one class).\n",
        "    - Two common metrics used are :         \n",
        "      - Gini Impurity:\n",
        "        - Measures the likelihood of an incorrect classification of a new instance of a random variable, if that new instance were randomly classified according to the distribution of class labels from the dataset. A Gini score of 0 denotes a pure node (all samples belong to the same class).\n",
        "      - Entropy (Information Gain):\n",
        "        - Measures the amount of randomness or uncertainty in the data. The algorithm calculates the Information Gain for each possible split and chooses the one that provides the highest gain (reduces entropy the most).$$Entropy(S) = - \\sum p_i \\log_2(p_i)$$Where $p_i$ is the probability of an element belonging to a specific class.  \n",
        "    3. Stopping Cirteria\n",
        "      - The tree continues to grow and split until one of the following conditions is met:\n",
        "        - Pure Node: All data points in a node belong to the same class.\n",
        "        - Max Depth: The tree reaches a pre-defined maximum depth (to prevent overfitting).\n",
        "        - Min Samples: The number of samples in a node is below a certain threshold.    "
      ],
      "metadata": {
        "id": "rPED1OIM94JQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "  - In the context of Decision Trees, Impurity is a measure of the \"homogeneity\" os the labels at a node. A node is \"pure\" if all its samples belong to the same class (e.g., all \"Yes\"). It is \"Impure\" if it contains a mix of different classes (e.g., 50% \"Yes\", 50% \"NO\").\n",
        "  - The goal of a decision tree is to split the data ia a way that minimize impurity in the child nodes. The two most common metrics to measures this are Gini Impurity and Entropy.\n",
        "  1. Gini Impurity\n",
        "  - Gini impurity measures the probability of misclassifying a randomly chosen element from the dataset if it were randomly labled according tio the class distribution in dataset.\n",
        "  - Formula:$$Gini = 1 - \\sum_{i=1}^{C} (p_i)^2$$Where $p_i$ is the probability of an element belonging to class $i$\n",
        "  - Range: It ranges from 0 to 0.5 (for binary calssification).\n",
        "  - 0:\n",
        "    - Perfectly pure node(all samples are the same class)\n",
        "  - 0.5:\n",
        "    - Maximum impurity (samples are evenly distributed across classes).\n",
        "  - Intuition:\n",
        "    - Gini is about \"how often would i be wrong if i guessed randomly based on the distribution?\" If a bag hass 99 red balls amd 1 blue ball, you are very likely to pick a red ball, and very likely to guess \"red,\" so your error (impurity) is low.\n",
        "  2. Entropy:\n",
        "    -  Entropy is a concept borrowed from Information Theory. It measures the amount of \"disorder,\" uncertainty, or surprise in the data.Formula:$$Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)$$Where $p_i$ is the probability of class $i$.\n",
        "  - Range: It ranges from 0 to 1 (for binary classification).\n",
        "  - 0 : Perfectly pure (zero disorder).\n",
        "  - 1 : Maximum impurity (maximum disorder/uncertainty, e.g., a 50/50 split).\n",
        "  - Intuition: High entropy means the dataset is chaotic and unpredictable. Low entropy means the dataset is orderly and predictable.\n",
        "  3. How They Impact Splits\n",
        "  - The decision tree algorithm (like CART for Gini or ID3/C4.5 for Entropy) doesn't just calculate impurity for a single node; it calculates the change in impurity (Information Gain) to decide where to split.\n",
        "  - Here is the step-by-step process of how they impact the tree construction:\n",
        "  - Calculate Parent Impurity:\n",
        "    -  The algorithm calculates the Gini or Entropy of the current node (Parent) before splitting.\n",
        "  - Test Possible Splits:\n",
        "    - It iterates through every possible feature and every possible threshold (e.g., \"Age > 25\", \"Age > 26\", \"Income > 50k\").\n",
        "  - Calculate Weighted Child Impurity:\n",
        "    - For each test split, it calculates the impurity of the resulting child nodes. It takes a weighted average based on the number of samples in each child.$$Weighted\\_Impurity = \\frac{N_{left}}{N_{total}} \\times Impurity_{left} + \\frac{N_{right}}{N_{total}} \\times Impurity_{right}$$\n",
        "    - Select the Best Split:\n",
        "     - For Entropy: It calculates Information Gain (Parent Entropy - Weighted Child Entropy). It chooses the split with the highest Information Gain.\n",
        "     - For Gini: It chooses the split that produces the lowest Weighted Gini Impurity.\n"
      ],
      "metadata": {
        "id": "6RKr9TApqCAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "  - Pruning is a technique used to overcome overfitting in Decision Trees\n",
        "  - Overfitting occurs when a tree becomes so complex that it \"memorizes\" the noise in the training data rather than learning the actual patterns, leading to poor performance on new data.\n",
        "  - Here is the difference between Pre-Pruning and Post-Pruning:\n",
        "  1. Pre-Pruning (Early Stopping) :       \n",
        "  Pre Pruning involves halting the growth of the decision tree before it perfectly classifies the training set. You stop the tree-building process early based on specific stopping criteria/hyperparameters.\n",
        "  - How it works:\n",
        "    - At each step of spliting, the algorithm checks a condtition. if the condition is met (e.g., the tree is too deep), it stops splliting that node and turns it into a leaf node, evem if the node ins't pure yet.\n",
        "  - Common Criteria:\n",
        "    - Max Depth: Stop if the tree reaches a depth of X.\n",
        "    - Min Sample Split: Stopm if a node has fewer than X samples.\n",
        "    - Min Impurity Decreases: Stop if splitting doesn't reduce impurity by at least X.    \n",
        "  2. Post-Punning (Backward-Punning)\n",
        "  Post-pruning involves allowing the tree to grow to its full extent (until all leaves are pure or contain very few samples), and then trimming back the branches that do not provide significant information.\n",
        "  - How it works:\n",
        "    - The algorithm builds a massive, overfitted tree first. Then, it works from the bottom up (from leaves to root). It replaces a subtree with a leaf node if removing that subtree does not significantly increase the error rate (often verified using a validation dataset or cross-validation).\n",
        "  - Common Method:\n",
        "    - Cost Complexity Pruning (used in scikit-learn via the ccp_alpha parameter). It assigns a penalty to the number of terminal nodes, finding the right balance between the tree's size and its accuracy.    \n",
        "  - Practical Advantages :   \n",
        "  - Advantage of Pre-Pruning: Computational Efficiency\n",
        "    - Why:\n",
        "      - It is significantly faster and uses less memory because you never build the full, massive tree. Practical Use Case: If you are working with a massive dataset (millions of rows) or in a real-time application where training speed is critical, pre-pruning (e.g., setting max_depth=10) ensures you get a \"good enough\" model quickly without exhausting system resources.\n",
        "  - Advantage of Post-Pruning: Higher Accuracy (Better Generalization)\n",
        "    - Why:\n",
        "      - It avoids the \"Horizon Effect.\" Sometimes a split early in the tree looks \"bad\" (low information gain) but opens the door to a very \"good\" split deeper down. Pre-pruning would stop early and miss this; Post-pruning builds the whole thing, sees the value of the deeper split, and keeps it. Practical Use Case: In competitions (like Kaggle) or medical diagnosis where accuracy is paramount and you can afford a longer training time, post-pruning usually yields a more robust and accurate model.  "
      ],
      "metadata": {
        "id": "q8sYgDYQ1NEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "  - Information Gain is the metric used by Decision Tree algorithms (specifically ID3 and C4.5) to decide which feature to split on at each step.\n",
        "  - In simple terms, it measures how much \"uncertainty\" (entropy) was removed from the dataset after splitting it on a specific attribute.\n",
        "  - The Concept\n",
        "  - Imagine you are playing a game of \"20 Questions.\n",
        "  - \"Question A:\n",
        "    - \"Is it alive?\" (Reduces the possibilities massively $\\rightarrow$ High Information Gain)\n",
        "  - Question B:\n",
        "    - \"Does its name start with the letter T?\" (Doesn't help much $\\rightarrow$ Low Information Gain)\n",
        "  - In a Decision Tree, the algorithm tests every feature and calculates how much information it \"gains\" by splitting on that feature. It then chooses the feature with the highest Information Gain to be the next node.\n",
        "  - The Formula\n",
        "  - Mathematically, Information Gain is simply the difference between the entropy of the parent node and the weighted average entropy of the child nodes.$$Information\\ Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)$$$Entropy(S)$: The impurity of the original dataset (Parent).\n",
        "  - $Entropy(S_v)$: The impurity of the new subset (Child) created by the split\n",
        "  - $\\frac{|S_v|}{|S|}$: The weight (proportion) of data points that ended up in that child node.\n",
        "  - Why Is It Important for Choosing the Best Split?\n",
        "    - Information Gain is the core \"selection criterion\" that drives the tree construction. Here is why it is critical:\n",
        "  1. . Identifies the Most Important Features\n",
        "    - It acts as a filter to find the most predictive features. The feature at the root of the tree (topmost node) is always the one with the highest Information Gain—meaning it is the single most important factor in determining the outcome.\n",
        "\n",
        "2. Minimizes Tree Depth (Efficiency)\n",
        "    - By choosing splits that reduce impurity the fastest, the algorithm builds a shallower, more efficient tree. If we chose splits with low information gain, we would need many more layers of questions to reach a pure leaf node.\n",
        "\n",
        "3. Prevents \"Bad\" Splits\n",
        "    - Consider a dataset of 100 people, 50 \"Fit\" and 50 \"Unfit\" (High Entropy/Uncertainty).\n",
        "\n",
        "    - Split by \"Gym Membership\": You get one group of 45 \"Fit\" / 5 \"Unfit\" and another of 5 \"Fit\" / 45 \"Unfit\". This drastically reduces uncertainty. (High Gain)\n",
        "\n",
        "    - Split by \"Favorite Color\": You get groups that are still roughly 50/50 mixed. The uncertainty remains high. (Low/Zero Gain)\n",
        "\n",
        "    - Information Gain ensures the algorithm picks \"Gym Membership\" over \"Favorite Color.\"\n",
        "\n",
        "  - A Known Drawback Bias toward Multi-Valued Attributes\n",
        "    - Information Gain has a flaw: it is biased towards attributes with a large number of distinct values.\n",
        "\n",
        "  - Example:\n",
        "    - If you have a \"User ID\" feature, splitting on it would result in perfectly pure nodes (1 user per leaf). This gives maximum Information Gain but is useless for prediction (overfitting).\n",
        "\n",
        "  - Solution:\n",
        "    - To fix this, algorithms like C4.5 use Gain Ratio, which penalizes attributes with too many branches\n"
      ],
      "metadata": {
        "id": "wiiK52Hs5PJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "  - Decision Trees are widely used because they mimic human decision-making and are easy to interpret. While more compplex algorithms (like neural Networks) often oytperform them in raw accuracy, Decision Trees are preferred in industries where exaplaining the \"why\" behind a preediction is legally or operrationally required.\n",
        "  - Common Real-World Applications\n",
        "    1. Finance: Credit Scoring & Loan Approval\n",
        "      - Application: Banks are use decision trees to determine if a loan applicant is \"low Risk\" or \"High Risk\".\n",
        "      - How it works: The tree splits users based on income, Credit History, Employment Starus, and Debt-to-income Ratio.\n",
        "      - Here why: REgulation often require banks to explain why a oan was rejected. A decision tree provides a clear audit trail(e.g., \"Rejected because income < r$30K AND Credit Score < 600\").\n",
        "    2. Healthcare: Triage & Diagnosis\n",
        "      - Application: Emergency rooms use decision trees (often as flowcharts) to prioritize patients or suggest initial treatments.\n",
        "      - How it works:\n",
        "        - Symptoms: Chest pain ? -->> Yes  \n",
        "        - Age: > 45? -->> Yes.\n",
        "        - History: Smokers? -->> Yes.\n",
        "        - Action: Prioritize (High Risk of Heart Attack).\n",
        "      - Why here:\n",
        "        - Doctors need a transparent tool that aligns with medical guidelines, not a \"black box\" AI they can't verify."
      ],
      "metadata": {
        "id": "qqy2NI-a-fr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "  -  Load the Iris Dataset\n",
        "  -  Train a Decision Tree Classifier using the Gini criterion\n",
        "  - Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "bhRjAf5eDz_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXJQi-4k9q9G",
        "outputId": "d697d99a-cbd2-45e3-d716-300cef497025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00 (100.0%)\n",
            "------------------------------\n",
            "Feature Importances:\n",
            "             Feature  Importance\n",
            "2  petal length (cm)    0.906143\n",
            "3   petal width (cm)    0.077186\n",
            "1   sepal width (cm)    0.016670\n",
            "0  sepal length (cm)    0.000000\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "# random_state ensures reproducible results\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree Classifier using the Gini criterion\n",
        "# 'criterion' is set to 'gini' by default, but we specify it explicitly here\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 3. Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f} ({accuracy*100:.1f}%)\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 4. Print Feature Importances\n",
        "print(\"Feature Importances:\")\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': iris.feature_names,\n",
        "    'Importance': clf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "  - Load the Iris Dataset\n",
        "  - Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
      ],
      "metadata": {
        "id": "kIkiLdb4E4mH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset (80% train, 20% test)\n",
        "# random_state=42 ensures the split is the same every time we run it\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Model 1: Fully Grown Tree ---\n",
        "# By default, max_depth=None, allowing the tree to grow until all leaves are pure\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# --- Model 2: Pruned Tree (max_depth=3) ---\n",
        "# We limit the depth to 3 levels to prevent overfitting\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "y_pred_pruned = pruned_tree.predict(X_test)\n",
        "acc_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# --- Output the Comparison ---\n",
        "print(f\"Accuracy of Fully Grown Tree: {acc_full:.4f}\")\n",
        "print(f\"Accuracy of Pruned Tree (depth=3): {acc_pruned:.4f}\")\n",
        "\n",
        "# Check depth of the full tree for context\n",
        "print(f\"Actual depth of the fully grown tree: {full_tree.get_depth()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBpBlbdrEbfM",
        "outputId": "b70038b4-2f73-4d9f-cc9a-e5683b37a6a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Fully Grown Tree: 1.0000\n",
            "Accuracy of Pruned Tree (depth=3): 1.0000\n",
            "Actual depth of the fully grown tree: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "  - Load the Boston Housing Dataset\n",
        "  - Train a Decision Tree Regressor\n",
        "  - Print the Mean Squared Error (MSE) and feature importances\n"
      ],
      "metadata": {
        "id": "d7k86F38FIVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code 1: Iris Classification (Accuracy & Feature Importance)\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree Classifier using the Gini criterion\n",
        "# Note: criterion='gini' is the default, but we specify it for clarity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 3. Print Accuracy and Feature Importances\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "# Create a simple DataFrame to display features alongside their importance scores\n",
        "importances = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': clf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(importances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itbQHOS5FBcG",
        "outputId": "c0261034-eff0-46c2-c0e7-7b3565da77c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "------------------------------\n",
            "Feature Importances:\n",
            "             Feature  Importance\n",
            "2  petal length (cm)    0.906143\n",
            "3   petal width (cm)    0.077186\n",
            "1   sepal width (cm)    0.016670\n",
            "0  sepal length (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code 2: Iris Depth Comparison (Pruning vs. Full Tree)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load and split data\n",
        "data = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train a Pruned Tree (max_depth=3)\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "y_pred_pruned = pruned_tree.predict(X_test)\n",
        "\n",
        "# 2. Train a Fully Grown Tree (max_depth=None)\n",
        "full_tree = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "\n",
        "# 3. Compare Accuracies\n",
        "print(f\"Accuracy (Max Depth = 3): {accuracy_score(y_test, y_pred_pruned):.4f}\")\n",
        "print(f\"Accuracy (Fully Grown):   {accuracy_score(y_test, y_pred_full):.4f}\")\n",
        "\n",
        "# Check actual depth of the full tree\n",
        "print(f\"Actual depth of full tree: {full_tree.get_depth()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce7i6y-0GOQx",
        "outputId": "3063038e-34cc-4e05-d624-e8d761ccd0c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Max Depth = 3): 1.0000\n",
            "Accuracy (Fully Grown):   1.0000\n",
            "Actual depth of full tree: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code 3: Boston Housing Regression (MSE & Feature Importance)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the Boston Housing Dataset (using fetch_openml as load_boston is deprecated)\n",
        "# data_id=531 is the ID for the Boston Housing dataset on OpenML\n",
        "boston = fetch_openml(data_id=531, parser='auto')\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree Regressor\n",
        "# We use a regressor here, not a classifier, because the target is a continuous value (price)\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# 3. Print Mean Squared Error (MSE) and Feature Importances\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "importances = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': regressor.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(importances.head()) # Printing top 5 features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LqlsZvQGXQ0",
        "outputId": "82b58669-413f-4411-a303-c56e8b1beab7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 10.42\n",
            "------------------------------\n",
            "Feature Importances:\n",
            "   Feature  Importance\n",
            "5       RM    0.600326\n",
            "12   LSTAT    0.193328\n",
            "7      DIS    0.070688\n",
            "0     CRIM    0.051296\n",
            "4      NOX    0.027148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "-  Load the Iris Dataset\n",
        "-  Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "-  Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "oUYgbbvtGpeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the Iris Dataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset (80% for training/tuning, 20% for final evaluation)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "UgsABXhDGgrE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, None],           # Try different depths\n",
        "    'min_samples_split': [2, 5, 10]         # Try different split thresholds\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# cv=5 means 5-fold Cross-Validation\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "WdaCjJI1GvvQ",
        "outputId": "ddc70dce-8df1-4561-809a-5a36b25a91b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42),\n",
              "             param_grid={'max_depth': [3, 5, 7, None],\n",
              "                         'min_samples_split': [2, 5, 10]},\n",
              "             scoring='accuracy')"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42),\n",
              "             param_grid={&#x27;max_depth&#x27;: [3, 5, 7, None],\n",
              "                         &#x27;min_samples_split&#x27;: [2, 5, 10]},\n",
              "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42),\n",
              "             param_grid={&#x27;max_depth&#x27;: [3, 5, 7, None],\n",
              "                         &#x27;min_samples_split&#x27;: [2, 5, 10]},\n",
              "             scoring=&#x27;accuracy&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: DecisionTreeClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=7, random_state=42)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=7, random_state=42)</pre></div> </div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.Print the best parameters and the resulting model accuracy\n",
        "# Execute the Grid Search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best score achieved during validation\n",
        "best_params = grid_search.best_params_\n",
        "best_cv_score = grid_search.best_score_\n",
        "\n",
        "# Evaluate the best model on the independent test set\n",
        "best_model = grid_search.best_estimator_\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "\n",
        "print(f\"Best Parameters Found: {best_params}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {best_cv_score:.4f}\")\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM0PdwjrGwX1",
        "outputId": "6385616b-4ade-4ddf-f842-a96a35ab84f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters Found: {'max_depth': 7, 'min_samples_split': 2}\n",
            "Best Cross-Validation Accuracy: 0.9417\n",
            "Test Set Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "-  Handle the missing values\n",
        "-  Encode the categorical features\n",
        "-  Train a Decision Tree model\n",
        "- Tune its hyperparameters\n",
        "- Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting."
      ],
      "metadata": {
        "id": "HXLPaoIVIW-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 1 : Data Preprocessing :\n",
        "  - Before the model can learn anything, the \"messy\" real-world data must be cleaned.\n",
        "  1. Handling Missing Values\n",
        "      - Missing data in healthcare is common (e.g., a patient didn't take a specific test).\n",
        "      - For Numerical Features(e.g., Blood Pressure, Age):\n",
        "      - Simple Imputation:\n",
        "        -  If the data is normally distributed, i would fill missing values with the Mean. if it is skewed(has outlires), i would use the Median.\n",
        "      - Advanced Method:\n",
        "        - Use KNN Imputer, which finds \"similar\" patients based on other features and uses their values to fill the gap.\n",
        "  2. Encoding Categorical Features\n",
        "      - Machine Learning models (specifically scikit-learn implementations) require numerical input.\n",
        "      - One-Hot Encoding: For nomianal variables with no inherent order (e.g., Gender, Blood Type). This creates binary columns(e.g., Is_Type_A, Is_Type_O).\n",
        "      - Label/ordinal Encoding: For ordinal varibales with a clear rank (e.g., pain level: Low/Medium/High.). I would map these to 0,1,2 to preserve the order.\n",
        "Phase 2 : Model Development\n",
        "  3. Train a Decision Tree Model\n",
        "      - Data Split: I would split the data into Training(70%), Validation(15%), and testing (15%) sets. Stratified sampling is crucial here to ensure the percentage of \"sick\" patients is consistent across all splits.\n",
        "      - Baseline Model: I would train an initial \"vanilla\" Decision Tree without constraints to establish a baseline performance and identify feature importance.\n",
        "  4. Tune hyperparameters\n",
        "      - A default Decision Tree will almost certainly overfit (memorize) the training data. I would use GridSearchCV or RandomizedSearchCV to find the optimal balance.\n",
        "      - max_depth:\n",
        "        - Limit how deep the tree grows to prevent it from learning noise.\n",
        "      - min_samples_leaf:\n",
        "        - Ensure every final decision node has a statistically significant number of patients(e.g., at least 20).\n",
        "      - class_weight:\n",
        "          - Since diseases are often rare (imbalaced data), I would set this to 'balanced' so the model pays more attention to minority class (the sick patients).\n",
        "Phase 3: Evaluation & Business Impact :      \n",
        "  - 5. Evaluate Performance\n",
        "    - In healthcare, Accuracy is misleading. (If 99% of patients are healthy, a model that predicts \"Healthy\" for everyone is 99% accurate but useless).\n",
        "  - I would focus on :      \n",
        "    - Recall(Sensitivity):   \n",
        "      - Out of all the people who actually have the disease, how many did we catch? This is the most critical metric. Missing a sick patient (False Negative) is life-threatening.\n",
        "    - Precision:\n",
        "      - Out of all people we predicted as sick, how many actually are? (Low precision causes \"alarm fatigue\" for doctors).\n",
        "    - ROC-AUC Score:\n",
        "      - To measure how well the model distinguishes between sick and healthy patients across different thresholds.\n",
        "   - 6. Deploying this model provides three key benefits:\n",
        "     - Early Intervention:\n",
        "      - By flagging high-risk patients who might be asymptomatic, the hospital can start treatment earlier, significantly improving survival rates.\n",
        "    - Resource Optimization (Triage):\n",
        "      - In a busy hospital, the model can act as a \"first pass\" filter, prioritizing high-risk patients for immediate doctor review while lower-risk patients wait.\n",
        "    - Cost Reduction:\n",
        "       - Preventative care is cheaper than emergency care. Catching the disease early prevents expensive surgeries or ICU stays later.                                     "
      ],
      "metadata": {
        "id": "J-O38cW9JvOr"
      }
    }
  ]
}